{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63641d2c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab5ddb96",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4237068877.py, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [6]\u001b[1;36m\u001b[0m\n\u001b[1;33m    +\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "file='qdata2/index.txt'\n",
    "with open(file,\"r\")as f:\n",
    "    lines=f.readlines()\n",
    "    \n",
    "\n",
    "\n",
    "#the numbers at the beginning are not useful hence we delete them also we convet all to lower-case\n",
    "\n",
    " \n",
    "def delete_intial_num(document_text):\n",
    "    # remove the leading numbers from the string, remove not alpha numeric characters, make everything lowercase\n",
    "    terms = [term.lower() for term in document_text.strip().split()[1:]]\n",
    "    return terms\n",
    "    \n",
    "#vocabulary is a dictionary having all words in the corpus with their number of apperance in the corpus\n",
    "vocabulary={}\n",
    "#document array contains all question titles without index\n",
    "document=[]\n",
    "for line in lines:   \n",
    "    tokens=delete_intial_num(line)\n",
    "    document.append(tokens)\n",
    "    tokens=set(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token]=1\n",
    "        else:\n",
    "            vocabulary[token]+=1\n",
    "            \n",
    "            \n",
    "#we need to sort the vocabulary in increasing order of values\n",
    "vocabulary=dict(sorted(vocabulary.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"Number of documents\",len(document))\n",
    "print('all different words',len(vocabulary))\n",
    "\n",
    "\n",
    "#IDF=no of apperance in corpus/total docs\n",
    "#hence the value in the key/total docs is an indication of IDF\n",
    "#save these keys in a file and save value of keys in another file\n",
    "\n",
    "\n",
    "#saving vocabulary in a text file\n",
    "with open('TF-IDF/vocab.txt','w')as f:\n",
    "    for key in vocabulary.keys():\n",
    "        f.write(\"%s\\n\" % key)\n",
    "        \n",
    " #saving IDF values in a text file\n",
    "with open('TF-IDF/idf_values.txt','w')as file:\n",
    "    for value in vocabulary.keys():\n",
    "        file.write(\"%s\\n\" % vocabulary[value])\n",
    "\n",
    "        \n",
    "#saving the document in a text file\n",
    "with open('TF-IDF/document_data.txt','w')as fi:\n",
    "     for doc in document:\n",
    "        fi.write(\"%s\\n\" % ' '.join(doc))\n",
    "  \n",
    "        \n",
    "        \n",
    "#for fast searching we use inverted index technique, here we store the index of documents a word is resent in\n",
    "inverted_index={}\n",
    "for index,lining in enumerate(document):\n",
    "    for token in lining:\n",
    "        if token not in inverted_index:\n",
    "            inverted_index[token]=[index]\n",
    "        else:\n",
    "            inverted_index[token].append(index)\n",
    " \n",
    "\n",
    "\n",
    " #saving the inverted index in a text file\n",
    "with open('TF-IDF/inverted1.txt', 'w') as f:\n",
    "    for key in inverted_index.keys():\n",
    "        f.write(\"%s\\n\" % key)\n",
    "        f.write(\"%s\\n\" % ' '.join([str(doc_id) for doc_id in inverted_index[key]]))        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e711e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47d7bce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents:  1237\n",
      "Sample document:  ['delete', 'greatest', 'value', 'in', 'each', 'row']\n",
      "Size of inverted index:  1392\n",
      "Enter your query: linked list\n",
      "['linked', 'list']\n",
      "linked {'61': 0.125, '83': 0.2, '130': 0.25, '301': 0.3333333333333333, '401': 0.16666666666666666, '458': 0.25, '508': 0.2, '592': 0.16666666666666666, '862': 0.2, '944': 0.14285714285714285, '1050': 0.3333333333333333, '1127': 0.16666666666666666, '1149': 0.3333333333333333} 4.555495014930951\n",
      "list {'61': 0.125, '83': 0.2, '130': 0.25, '206': 0.14285714285714285, '301': 0.3333333333333333, '319': 0.25, '401': 0.16666666666666666, '458': 0.25, '508': 0.2, '592': 0.16666666666666666, '640': 0.3333333333333333, '939': 0.16666666666666666, '944': 0.14285714285714285, '990': 0.5, '1050': 0.3333333333333333, '1127': 0.16666666666666666, '1129': 0.14285714285714285, '1149': 0.3333333333333333} 4.230072614496323\n",
      "{'61': 1.667632830544778, '83': 2.668212528871645, '130': 3.335265661089556, '301': 4.447020881452742, '401': 2.223510440726371, '458': 3.335265661089556, '508': 2.668212528871645, '592': 2.223510440726371, '862': 1.8221980059723806, '944': 1.9058660920511747, '1050': 4.447020881452742, '1127': 2.223510440726371, '1149': 4.447020881452742, '206': 1.208592175570378, '319': 2.1150363072481615, '640': 2.8200484096642153, '939': 1.4100242048321077, '990': 4.230072614496323, '1129': 1.208592175570378}\n",
      "Document:  ['linked', 'list', 'cycle']  Score:  2.223510440726371\n",
      "Document:  ['reverse', 'linked', 'list']  Score:  2.223510440726371\n",
      "Document:  ['palindrome', 'linked', 'list']  Score:  2.223510440726371\n",
      "Document:  ['rotate', 'list']  Score:  2.1150363072481615\n",
      "Document:  ['remove', 'linked', 'list', 'elements']  Score:  1.667632830544778\n",
      "Document:  ['odd', 'even', 'linked', 'list']  Score:  1.667632830544778\n",
      "Document:  ['insertion', 'sort', 'list']  Score:  1.4100242048321077\n",
      "Document:  ['middle', 'of', 'the', 'linked', 'list']  Score:  1.3341062644358226\n",
      "Document:  ['remove', 'nodes', 'from', 'linked', 'list']  Score:  1.3341062644358226\n",
      "Document:  ['swapping', 'nodes', 'in', 'a', 'linked', 'list']  Score:  1.1117552203631855\n",
      "Document:  ['next', 'greater', 'node', 'in', 'linked', 'list']  Score:  1.1117552203631855\n",
      "Document:  ['delete', 'node', 'in', 'a', 'linked', 'list']  Score:  1.1117552203631855\n",
      "Document:  ['flatten', 'nested', 'list', 'iterator']  Score:  1.0575181536240807\n",
      "Document:  ['maximum', 'twin', 'sum', 'of', 'a', 'linked', 'list']  Score:  0.9529330460255874\n",
      "Document:  ['intersection', 'of', 'two', 'linked', 'lists']  Score:  0.9110990029861903\n",
      "Document:  ['remove', 'zero', 'sum', 'consecutive', 'nodes', 'from', 'linked', 'list']  Score:  0.833816415272389\n",
      "Document:  ['remove', 'duplicates', 'from', 'sorted', 'list', 'ii']  Score:  0.7050121024160538\n",
      "Document:  ['list', 'the', 'products', 'ordered', 'in', 'a', 'period']  Score:  0.604296087785189\n",
      "Document:  ['remove', 'nth', 'node', 'from', 'end', 'of', 'list']  Score:  0.604296087785189\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def load_vocabulary():\n",
    "    vocab = {}\n",
    "    with open('TF-IDF/vocab.txt', 'r') as f:\n",
    "        vocab_terms = f.readlines()\n",
    "    with open('TF-IDF/idf_values.txt', 'r') as f:\n",
    "        idf_values = f.readlines()\n",
    "    \n",
    "    for (term,idf_value) in zip(vocab_terms, idf_values):\n",
    "        vocab[term.strip()] = int(idf_value.strip())\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def load_document():\n",
    "    documents = []\n",
    "    with open('TF-IDF/document_data.txt', 'r') as f:\n",
    "        documents = f.readlines()\n",
    "    documents = [document.strip().split() for document in documents]\n",
    "\n",
    "    print('Number of documents: ', len(documents))\n",
    "    print('Sample document: ', documents[0])\n",
    "    return documents\n",
    "\n",
    "def load_inverted_index():\n",
    "    inverted_index={}\n",
    "    with open('TF-IDF/inverted1.txt', 'r') as f:\n",
    "        inverted_index_terms = f.readlines()\n",
    "\n",
    "    for row_num in range(0,len(inverted_index_terms),2):\n",
    "        term = inverted_index_terms[row_num].strip()\n",
    "        documents = inverted_index_terms[row_num+1].strip().split()\n",
    "        inverted_index[term] = documents\n",
    "    \n",
    "    print('Size of inverted index: ', len(inverted_index))\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "\n",
    "vocabularies=load_vocabulary()#dictionary having terms and their idf values\n",
    "docs=load_document()#name of the question as a string\n",
    "inverted_indices=load_inverted_index()#dictionary of words mapped into its the docs it is present in\n",
    "\n",
    "#get_tf_dictinary returns the tf value of the term\n",
    "def get_tf_dictionary(term):\n",
    "    tf_values = {}\n",
    "    if term in inverted_indices:\n",
    "        for document in inverted_indices[term]:\n",
    "            if document not in tf_values:\n",
    "                tf_values[document] = 1\n",
    "            else:\n",
    "                tf_values[document] += 1\n",
    "                \n",
    "    for document in tf_values:\n",
    "        tf_values[document] /= len(docs[int(document)])\n",
    "    \n",
    "    return tf_values\n",
    "\n",
    "#return the log(number of documents/idf value obtained from idf.txt)\n",
    "def get_idf_value(term):\n",
    "     return math.log(len(docs)/vocabularies[term])\n",
    "\n",
    "    \n",
    "    \n",
    "def calculate_sorted_order_of_docs(query_terms):\n",
    "    potential_documents = {}\n",
    "    for term in query_terms:\n",
    "        if vocabularies[term] == 0:\n",
    "            continue\n",
    "        tf_values_by_document = get_tf_dictionary(term)\n",
    "        idf_value = get_idf_value(term)\n",
    "        print(term,tf_values_by_document,idf_value)\n",
    "        for document in tf_values_by_document:\n",
    "            if document not in potential_documents:\n",
    "                potential_documents[document] = tf_values_by_document[document] * idf_value\n",
    "            potential_documents[document] += tf_values_by_document[document] * idf_value\n",
    "\n",
    "    print(potential_documents)\n",
    "    # divite by the length of the query terms\n",
    "    for document in potential_documents:\n",
    "        potential_documents[document] /= len(query_terms)\n",
    "\n",
    "    potential_documents = dict(sorted(potential_documents.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    for document_index in potential_documents:\n",
    "        print('Document: ', docs[int(document_index)], ' Score: ', potential_documents[document_index])\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "#we take input of a string and calculate sorted order of documents from the corpus\n",
    "query_string = input('Enter your query: ')\n",
    "query_terms = [term.lower() for term in query_string.strip().split()]\n",
    "\n",
    "print(query_terms)\n",
    "calculate_sorted_order_of_docs(query_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee52a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
